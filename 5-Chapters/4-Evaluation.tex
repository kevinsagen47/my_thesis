%----------------------------------------------------------------------
% 實驗設計分析
%----------------------------------------------------------------------

\chapter{Evaluation}\label{sec:evalutaion}
\section{Experiment }\label{sec:3-experiment}
\subsection{Experiment Setup}\label{sec:3-setup}
Our experiments aim to illustrate how the strengths of one sensor compensate for weaknesses in the other, 
demonstrating the potential of sensor fusion.
The weakness inherent in radar lies in its challenge to recognize objects due to sparse measurements. 
Although a camera offers high-resolution measurements, 
it lacks depth perception. 
However, radar compensates for this limitation by providing highly accurate range measurements, 
effectively mitigating the shortcomings of the camera.
To test the performance of the algorithm, 3 specific scenarios were tested:
1. Control scenario, when object 1 (white) and object 2 (yellow) do not crosspath.
2. One object conceals another and continues their original trajectory after departing.
3. One object conceals another and changes trajectory when departing.
\begin{figure}[!htb]
    \centering
    \begin{subfigure}{0.25\linewidth}
        \includegraphics[width=5.5cm]{Figures/scenario_1_gt.png}
        \caption{scenario 1}
        \label{subfig:scenario1gt}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\linewidth}
        \centering
        \includegraphics[width=5.5cm]{Figures/scenario_2_gt.png}
        \caption{scenario 2}
        \label{subfig:scenario2gt}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\linewidth}
        \centering
        \includegraphics[width=5.5cm]{Figures/scenario_3_gt.jpg}
        \caption{scenario 3}
        \label{subfig:scenario3gt}
    \end{subfigure}

    \caption{Bird Eye View of multi-object tracking}
    \label{fig:ground_truth}
\end{figure}

\subsection{Challenges to Overcome}\label{sec:3-challenge}
Blind spot of both sensors.
Challenges the algorithm to correctly predict objects when data is obstructed.

\begin{figure}[!htb]
    \centering
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=7cm]{Figures/before_conceal_radar.png}
        \caption{Raw radar data}
        \label{subfig:before_conceal_radar_fig}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=7cm]{Figures/before_conceal_image.png}
        \caption{Image frame}
        \label{subfig:before_conceal_image_fig}
    \end{subfigure}

    \caption{Object 1 and object 2 before crossingpath}
    \label{fig:before_conceal_fig}
\end{figure}

From figure \ref*{fig:concealing_fig}\subref{subfig:concealing_radar_fig} can be seen that both object clusters disappear.



\begin{figure}[!htb]
    \centering
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=7cm]{Figures/concealing_radar.png}
        \caption{Raw radar data}
        \label{subfig:concealing_radar_fig}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=7cm]{Figures/concealing_image.png}
        \caption{Image frame}
        \label{subfig:concealing_image_fig}
    \end{subfigure}

    \caption{Object 1 covers object 2}
    \label{fig:concealing_fig}
\end{figure}

\begin{figure}[!htb]
    \centering
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=7cm]{Figures/after_conceal_radar.png}
        \caption{Raw radar data}
        \label{subfig:after_conceal_radar_fig}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=7cm]{Figures/after_conceal_image.png}
        \caption{Image frame}
        \label{subfig:after_conceal_image_fig}
    \end{subfigure}

    \caption{Object 1 and object 2 seperates}
    \label{fig:after_conceal_fig}
\end{figure}





%\vspace*{5cm}
\newpage 
\section{Experiment Result}\label{sec:3-exp_result}
\subsection{Scenario 1}\label{sec:3-exp_result1}
Scenario 1 is the control when object 1 and object 2 do not crosspath.
%\href{https://drive.google.com/file/d/1SL30CC6EpyI4NLOcGnfALKAP44P82u9n/view?usp=sharing}{\color{blue}{Video}}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Radar Only Object Tracking Scenario 1.png}
        \caption{Radar Only Object Tracking Scenario 1}
        \label{subfig:radar_1}
    \end{subfigure}
    \hspace{0.1\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Radar vs Camera Azimuth Angle Scenario 1.png}
        \caption{Raw radar and image data after data association}
        \label{subfig:raw_fusion_1}
    \end{subfigure}

    \caption{Raw Data of Scenario 1}
    \label{fig:raw_1}
\end{figure}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/EKF Output in Cartesian Scenario 1.png}
        \caption{EKF Output in Cartesian Scenario 1}
        \label{subfig:ekf_cart_1}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/EKF Output Azimuth Scenario 1.png}
        \caption{EKF Output Azimuth Scenario 1}
        \label{subfig:ekf_azi_1}
    \end{subfigure}

    \caption{EKF Output of Scenario 1}
    \label{fig:ekf_1}
\end{figure}


\newpage 
\subsection{Scenario 2}\label{sec:3-exp_result2}
In scenario 2, object 1 crosses path with object 2 in a straight line horizontally.
After concealing object 2 from both camera and radar, both objects continue their original trajectory.
In figure \ref*{fig:raw_2}\subref{subfig:radar_2} can be seen that object 1 and object 2 switch places.
The algorithm is able to track and identify objects 1 and 2 correctly (figure \ref*{fig:ekf_2}\subref{subfig:ekf_cart_2}).
%\href{https://drive.google.com/file/d/1YnliV7YRzahYNpIehctzrNrf0Z0ZpIeY/view?usp=sharing}{\color{blue}{Video}}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Radar Only Object Tracking Scenario 2.png}
        \caption{Radar Only Object Tracking Scenario 2}
        \label{subfig:radar_2}
    \end{subfigure}
    \hspace{0.1\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Radar vs Camera Azimuth Angle Scenario 2.png}
        \caption{Raw radar and image data after data association}
        \label{subfig:raw_fusion_2}
    \end{subfigure}

    \caption{Raw Data of Scenario 2}
    \label{fig:raw_2}
\end{figure}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/EKF Output in Cartesian Scenario 2.png}
        \caption{EKF Output in Cartesian Scenario 2}
        \label{subfig:ekf_cart_2}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/EKF Output Azimuth Scenario 2.png}
        \caption{EKF Output Azimuth Scenario 2}
        \label{subfig:ekf_azi_2}
    \end{subfigure}

    \caption{EKF Output of Scenario 2}
    \label{fig:ekf_2}
\end{figure}


\newpage 
\subsection{Scenario 3}\label{sec:3-exp_result3}
In scenario 3, object 1 covers object 2, and in the next few frames, object 2 covers object 1.
In this scenario, both objects change trajectory when departing from each other.
In figure \ref*{fig:raw_3}\subref{subfig:radar_3} can be seen that object 1 and object 2 switch places.
The algorithm is able to track and identify objects 1 and 2 correctly (figure \ref*{fig:ekf_3}\subref{subfig:ekf_cart_3}).
%\href{https://drive.google.com/file/d/1bd92Bu6CJeL1QIIbBlW42cAozxEcis-d/view?usp=sharing}{\color{blue}{Video}}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.35\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Radar Only Object Tracking Scenario 3.png}
        \caption{Radar Only Object Tracking Scenario 3}
        \label{subfig:radar_3}
    \end{subfigure}
    \hspace{0.1\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.35\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Radar vs Camera Azimuth Angle Scenario 3.png}
        \caption{Raw radar and image data after data association}
        \label{subfig:raw_fusion_3}
    \end{subfigure}

    \caption{Raw Data of Scenario 3}
    \label{fig:raw_3}
\end{figure}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/EKF Output in Cartesian Scenario 3.png}
        \caption{EKF Output in Cartesian Scenario 3}
        \label{subfig:ekf_cart_3}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/EKF Output Azimuth Scenario 3.png}
        \caption{EKF Output Azimuth Scenario 3}
        \label{subfig:ekf_azi_3}
    \end{subfigure}

    \caption{EKF Output of Scenario 3}
    \label{fig:ekf_3}
\end{figure}


\newpage



\section{Object Tracking Performance Evaluation}\label{sec:3-evaluation}
\begin{equation}\label{equ:4_RMSE}
\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
\end{equation}

\subsection{3D Bounding Box Error Evaluation}\label{sec:3-bbox}
The accuracy of the 3D Bounding Box depends on the precision of both the radar's range and, notably, 
Yolo's Bounding Box (BBox). 
Additionally, the accuracy of the width and height of the bounding box is contingent upon capturing the subject of interest entirely within the camera frame. 
In our scenarios, the accuracy of the subjects' height is reliable only when they are 3.7 meters or farther away. 
Width prediction performance is not analyzed since our setup does not have a reliable way to collect subjects' widths.
The tables below demonstrate that the algorithm can predict the subject's height with accuracy within 0.1 meters throughout the experiment.
\begin{table}[h!]
    \begin{center}
      \label{tab:table3}
      \begin{tabular}{c|c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
        \multirow{4}{*}{\textbf{Scenario}} & \multicolumn{4}{c}{\textbf{Height (m)}}\\\cline{2-5}
                                            & \multicolumn{2}{c|}{\textbf{Object 1}}  & \multicolumn{2}{c}{\textbf{Object 2}}\\
                                            & \multicolumn{2}{c|}{Ground Truth = 1.67m }& \multicolumn{2}{c}{Ground Truth = 1.80m}\\
                                            \cline{2-5}
                                            & Height & Covariance & Height & Covariance \\
        \hline
        Scenario 1                          & 1.6954 & 0.0072 & 1.8190 & 0.0333   \\
        Scenario 2                          & 1.7677 & 0.0061 & 1.9494 & 0.2058  \\
        Scenario 3                          & 1.7583 & 0.1554 & 1.8470 & 0.0932  \\
      \end{tabular}
    \end{center}
    \caption{All scenarios BBox prediction compared}
    \label{tab:bbox_table}
  \end{table}


\subsection{Azimuth Error Evaluation}\label{sec:3-theta}
The table presents azimuth errors (Root Mean Square Error - RMSE) for three different scenarios, 
considering two objects in each scenario.
The RMSE (rad) columns provide a comparison of errors in radians 
for the fusion of radar and camera data, 
as well as individual errors for camera-only and radar-only measurements.
Despite the difference in theta accuracy between radar and camera, 
the algorithm effectively leverages the higher accuracy of the camera's angle while incorporating data from both sources.
The "Fusion" column represents the azimuth error when the algorithm fuses data from both radar and camera. 
In some cases (e.g., Scenario 1), the fusion result outperforms both individual camera and radar measurements, 
showcasing the benefits of integrating information from both sources.
In some cases, the fusion result can lag behind the camera's result, but not by a large margin.
\begin{table}[h!]
    \begin{center}
      \label{tab:table2}
      \begin{tabular}{c|c|c|c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
        \multirow{3}{*}{\textbf{Scenario}} & \multicolumn{6}{c}{\textbf{RMSE (rad)}}\\\cline{2-7}
                                            & \multicolumn{3}{c|}{\textbf{Object 1}}  & \multicolumn{3}{c}{\textbf{Object 2}}\\
        \cline{2-7}
                   & Fusion & Camera & Radar & Fusion & Camera & Radar \\
        \hline
        Scenario 1 & \textbf{0.0242} & 0.0312 & 0.0358 & \textbf{0.0443}  & 0.0977 & 0.0493 \\
        Scenario 2 & 0.0566 & \textbf{0.0555} & 0.1228 & \textbf{0.0644} & 0.0917 & 0.0749 \\
        Scenario 3 & 0.0512 & \textbf{0.0366} & 0.1158 & 0.0672 & \textbf{0.0611} & 0.1537 \\
      \end{tabular}
    \end{center}
    \caption{All scenarios azimuth error compared}
    \label{tab:scenarios_rmse}
  \end{table}

  
\subsection{Cartesian Error Evaluation}\label{sec:3-cart}
From Scenario 1, it becomes apparent that all components—fusion and radar commendable performance. 
In scenario 2, where the algorithm showcases its strengths, it combines the best of both sensors to produce better a result.
Conversely, in Scenario 3, radar struggles significantly in tracking object 2 as it enters its blind spot. 
Nevertheless, the algorithm maintains its performance by incorporating camera data. 
The collective analysis across all scenarios yields an average RMSE of 0.1561m.
\begin{table}[h!]
    \begin{center}
      \label{tab:table5}
      \begin{tabular}{c|c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
        \multirow{3}{*}{\textbf{Scenario}} & \multicolumn{4}{c}{\textbf{RMSE (m)}}\\\cline{2-5}
                                            & \multicolumn{2}{c|}{\textbf{Object 1}}  & \multicolumn{2}{c}{\textbf{Object 2}}\\
        \cline{2-5}
                   & Fusion & Radar & Fusion & Radar \\
        \hline
        Scenario 1 & 0.1388  & \textbf{0.1124} & \textbf{0.105}   & 0.1201 \\
        Scenario 2 & \textbf{0.1206}  & 0.1228 & \textbf{0.1687}  & 0.2014 \\
        Scenario 3 & \textbf{0.0903}  & 0.0928 & \textbf{0.3075}  & 0.3592 \\
      \end{tabular}
    \end{center}
    \caption{Cartesian RMSE compared}
    \label{tab:cart_rmse}
  \end{table}

Comparison of errors between the author's proposed method and other radar-camera tracking algorithms.
Please note that the author did not benchmark the results of other algorithms with the same dataset.
  \begin{table}[h!]
    \begin{center}
      \label{tab:table1}
      \begin{tabular}{l|c|c|c|l} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
        \textbf{Method} & \textbf{Radar} & \textbf{Object} & \textbf{Max Range} & \textbf{RMSE} \\% %object detected
        \hline
        Proposed                           & AWR1843 & human   & 6m  & \textbf{0.1561 m}  \\%& 6m 
        \citeauthor{9081940}\cite{9081940} &  RT3002 & vehicle & 40m & 0.18 m \\%
        \citeauthor{method1}\cite{method1} & AWR1642 & vehicle & 50m & 0.1828 m\\%& 50m
        \citeauthor{8932892}\cite{8932892} & IWR1443 & human   & 6m  & 0.2486 m\\%
        \citeauthor{8844649}\cite{8844649} & AWR1642 & human   & 10m & 0.2902 m \\%
        
      \end{tabular}
    \end{center}
    \caption{Comparison of other tracking method}
    \label{tab:method_rmse}
  \end{table}

  \newpage 
  \begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Fused Scenario 1 Cartesian Error.png}
        \caption{Fused Scenario 1 Cartesian Error}
        \label{subfig:err_cart_1}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.3\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Fused Scenario 1 Azimuth Error.png}
        \caption{Fused Scenario 1 Azimuth Error}
        \label{subfig:err_azi_1}
    \end{subfigure}

    \caption{EKF Error of Scenario 1}
    \label{fig:error_1}
\end{figure}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.35\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Fused Scenario 2 Cartesian Error.png}
        \caption{Fused Scenario 2 Cartesian Error}
        \label{subfig:err_cart_2}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.35\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Fused Scenario 2 Azimuth Error.png}
        \caption{Fused Scenario 2 Azimuth Error}
        \label{subfig:err_azi2}
    \end{subfigure}

    \caption{EKF Error of Scenario 2}
    \label{fig:error_2}
\end{figure}
\begin{figure}[!htb]
    \hspace{0.1\textwidth}
    \begin{subfigure}[b]{0.35\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Fused Scenario 3 Cartesian Error.png}
        \caption{Fused Scenario 3 Cartesian Error}
        \label{subfig:err_cart_3}
    \end{subfigure}
    \hspace{0.15\textwidth}
    %\hfill
    \begin{subfigure}[b]{0.35\textwidth}%{0.25\linewidth}
        \includegraphics[width=7cm]{Figures/matlab/Fused Scenario 3 Azimuth Error.png}
        \caption{Fused Scenario 3 Azimuth Error}
        \label{subfig:err_azi_3}
    \end{subfigure}

    \caption{EKF Error of Scenario 3}
    \label{fig:error_3}
\end{figure}