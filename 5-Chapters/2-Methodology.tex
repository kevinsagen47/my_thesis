
\chapter{Methodology}\label{chap:related_works}


\section{Configuration}\label{sec:2-spec}
In this experimental setup, two fundamental sensors are employed to acquire and analyze data.
The Realsense D435i camera, 
known for its high-resolution image capture and depth-sensing capabilities, is utilized for visual data acquisition. 
Complementing the camera is the AWR1843boost mmWave radar, operating in the 77-81GHz frequency range. 
Both sensors are securely housed within a custom 3D-printed enclosure, 
which not only safeguards them but also minimizes external interference, ensuring the integrity of data acquisition. 
The mmWave radar config that is used is a 77-81GHz chirp, with settings balanced between range and resolution,
collecting data at 20 frames per second.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=10cm]{Figures/radar_camera_setup.png}%\textwidth
    \caption{Radar camera setup}
    \label{fig:radar_camera_setup_fig}
\end{figure}


\section{Overview}\label{sec:2-overview}
Overview block diagram of the project  
\begin{figure}[hpbt]
    \centering
    \includegraphics[width=12cm]{Figures/kf_update-modified.png}%\textwidth
    \caption{Radar camera Kalman Filter workflow}
    \label{fig:kf_update}
\end{figure}

\section{Calibration}\label{sec:2-calibration}
\subsection{Camera Calibration}
To accurately map the monocular camera's image coordinates to real-world coordinates, calibration of intrinsic and extrinsic is required.
Using tools provided by ROS \cite{cam_calib}.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=8cm]{Figures/cam_calib.png}%\textwidth
    \caption{Camera extrinsic and intrinsic calibration}
    \label{fig:camera_calibration}
\end{figure}

\subsection{Radar-Camera Calibration}
In order to achieve accurate sensor fusion, it is essential to conduct proper calibration of the two sensors. 
For this purpose, a corner reflector is employed, primarily due to its strong radar reflection characteristics. 
Additionally, it offers the advantage of appearing as a single point in both radar and camera data, effectively reducing ambiguity.

As illustrated in Figure <insert figure>, data points from both radar 
and camera coordinates can be collected with the corner reflector positioned at various locations.
\begin{figure}[hbpt]
    \centering
    \begin{subfigure}{0.25\linewidth}
        \includegraphics[width=5.5cm]{Figures/corner_reflector.jpg}
        \caption{Radar Corner Reflector}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\linewidth}
        \centering
        \includegraphics[width=5.5cm]{Figures/camera_corner.png}
        \caption{Camera-corner reflector calibration}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\linewidth}
        \centering
        \includegraphics[width=5.5cm]{Figures/radar_corner.png}
        \caption{Radar-corner reflector calibration}
    \end{subfigure}

    \caption{Radar camera calibration}
    \label{fig:radar_camera_calibration}
\end{figure}

\section{Data Pre-Processing}\label{sec:2-preprocessing}
\subsection{mmWave Radar Data Pre-Processing}\label{sec:2-kd_tree}
explain about kd-tree

\subsection{Image Recognition and Tracking}\label{sec:2-img_recognition}
explain about yolov3 and deepsort

\section{Radar-camera Data Association}\label{sec:2-association}
how to determine if cluster and image belong to the same object
resolution of theta 0.047
\begin{equation}\label{equ:angular_resolution}
    \Delta \theta= \frac{c_0}{f_c d N_{RX} N_{TX} \cos(\theta _i)}
\end{equation}
where
\begin{align*}
    f_c & = \text{center frequency} \\
    \lambda & = \text{carrier signal wavelength} \\
    d & =  \lambda/2 \\
    N_{RX} & = \text{Number of receiving antenna}\\
    N_{TX}& = \text{Number of transferring antenna}\\
    \theta _i &= \text{angle of interest}
\end{align*}

\section{Bayes Fusion}\label{sec:2-bayes_fusion}

Bayes fusion kalman filter 
\begin{equation}\label{equ:bayes1}
    P_{prob}(\frac{P}{X})=
    \frac
    {e \frac{−(P−X)^T R^{−1}(P−X)}{2}}
    {2 \pi R^(0.5)}
\end{equation}

\begin{equation}\label{equ:bayes1}
P
\end{equation}

\begin{equation}\label{equ:bayes2}
    P
\end{equation}

\begin{equation}\label{equ:bayes3}
    \frac{1}{R}=\frac{1}{R_1}+\frac{1}{R_2}
\end{equation}
\begin{equation}\label{equ:bayes4}
    y
\end{equation}
\begin{equation}\label{equ:bayes5}
    y
\end{equation}

\begin{equation}\label{equ:2-radar_R}
    \mathbf{R}_{radar} = 
    \begin{bmatrix}
        \sigma_{radar_x}^2 & 0 \\
        0 & \sigma_{radar_y}^2 \\
      \end{bmatrix}
\end{equation}
\begin{equation}\label{equ:2-R_cam}
    \mathbf{R}_{cam} = 
    \begin{bmatrix}
        \sigma_{cam_u}^2
    \end{bmatrix}
\end{equation}


\section{Multimodal Kalman Filter}\label{sec:2-kalman_filter}
\subsection{Predict}\label{sec:2-predict}
The state matrix used in this Kalman Filter is from the single sensor maneuvering tracking, with constant velocity.
It has four elements and is defined with position and velocity, which projects onto the x-axis and y-axis:

\begin{equation}\label{equ:state_eq}
    \mathbf{x} = 
        \begin{bmatrix} 
        p \\ 
        v 
        \end{bmatrix} = 
        \begin{bmatrix} 
        p_x \\ 
        p_y \\ 
        v_x \\ 
        v_y 
        \end{bmatrix}
\end{equation}
where
\begin{align*}
    p_x &=\text{position x}\\
    p_y &=\text{position y}\\
    v_x &=\text{velocity x}\\
    v_y &=\text{velocity y}\\
\end{align*}

\begin{equation}\label{equ:transition_matrix_H}
    \mathbf{F} = 
    \begin{bmatrix}
        1 & 0 & 1 & 0 \\
        0 & 1 & 0 & 1 \\
        0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 \\
      \end{bmatrix}
\end{equation}

\begin{equation}\label{equ:predict_eq}
    \mathbf{x}_k=\mathbf{F}_k\mathbf{x}_{k-1}+\mathbf{w}
\end{equation}

Error covariance update
\begin{equation}\label{equ:error_covariance}
    \mathbf{P}_k=\mathbf{F}_k \mathbf{P}_{k-1} \mathbf{F}_k^T+\mathbf{Q}_k
\end{equation}

\subsection{Update}\label{equ:2-update}
To fuse two sensors with different update rates, Kalman Filter is updated seperately
\subsubsection{Radar Update}\label{sec:2-radar_update}  
Centroid of radar
\begin{equation}
    \mathbf{z}_{radar}=
    \begin{bmatrix}
        p_x \\ 
        p_y
    \end{bmatrix}
\end{equation}

Transition matrix
\begin{equation}\label{equ:2-radar_transition_matrix}
    \mathbf{H} = 
    \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
      \end{bmatrix}
\end{equation}



\subsubsection{Camera Update}\label{sec:2-camera_update}
Center of bounding box.
Only the horizontal image coordinate is fused with Kalman Filter.

Conversion image to real-world vector (insert figure)
\begin{equation}\label{equ:img2cart}
u=c_x-\frac{p_x}{p_y}f
\end{equation}
where
\begin{align*}
    c_x &=\text{center of camera image in pixel}\\
    p_x &=\text{x-axis position in cartesian}\\
    p_y &=\text{y-axis position in cartesian}\\
    f &=\text{camera focal length in pixel}\\
    u &=\text{center of Bbox object detection in pixel}
\end{align*}

From equation \ref{equ:img2cart} we can obtain
\begin{equation}\label{equ:2-img2cart2}
    u=640-\frac{p_x}{p_y}950
\end{equation}

Thus
\begin{equation}\label{equ:2-z_cam}
    \mathbf{z}_{cam}=
    \begin{bmatrix}c_x-u\end{bmatrix}=
    \begin{bmatrix}640-u\end{bmatrix}
\end{equation}

\begin{equation}\label{equ:2-cam_transition_matrix}
    \mathbf{H} = 
    \begin{bmatrix}
        \frac{950}{p_y} & 0 & 0 & 0 \\
      \end{bmatrix}
\end{equation}



\begin{algorithm}[htbp]
    \SetAlgoNoLine

    \caption{演算法A}
    \label{algo:algoexample}

    \Input{
        長度為$n$的序列$S$
    }

    \Output{
        序列$S$的總和
    }

    % 這是在Algorithm加一條線R
    \AlgoHRule

    $i \gets 0$\;
    $x \gets 0$\;
    \;
    \While{$i < n$}
    {
        $x \gets x + S[i]$\;
        $i  \gets i + 1$\;
    }
    \BlankLine
    \Return $x$\;
\end{algorithm}