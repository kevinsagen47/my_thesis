\chapter{Methodology}\label{chap:Methodology}


\section{Configuration}\label{sec:2-spec}
In this experimental setup, two fundamental sensors are employed to acquire and analyze data.
The Realsense D435i camera, 
known for its high-resolution image capture and depth-sensing capabilities, is utilized for visual data acquisition. 
Complementing the camera is the AWR1843boost mmWave radar, operating in the 77-81GHz frequency range. 
Both sensors are securely housed within a custom 3D-printed enclosure as seen in figure \ref{fig:radar_camera_setup_fig}, 
which not only safeguards them but also minimizes external interference, ensuring the integrity of data acquisition. 
The mmWave radar config that is used is a 77-81GHz chirp, with settings balanced between range and resolution,
collecting data at 20 frames per second.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=5cm]{Figures/radar_camera_setup.png}%\textwidth
    \caption{Radar camera setup}
    \label{fig:radar_camera_setup_fig}
\end{figure}

\newpage
\section{Overview}\label{sec:2-overview}
Overview block diagram of the algorithm shown in figure \ref*{fig:kf_update}.
\begin{figure}[hpbt]
    \centering
    \includegraphics[width=10cm]{Figures/kf_update-modified.png}%\textwidth
    \caption{Radar camera Kalman Filter workflow}
    \label{fig:kf_update}
\end{figure}

\newpage
\section{Calibration}\label{sec:2-calibration}
\subsection{Homography}\label{subsec:2-1-Homography}
Accurate fusion of both sensors requires the transformation of their measurements. 
In this scenario, given the challenges in mapping image coordinates to polar coordinates, 
homography method mentioned in this paper \cite{KIM2014641} is employed.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=12cm]{Figures/homography.png}%\textwidth
    \caption{Homography of two coordinate system}
    \label{fig:homography_figure}
\end{figure}
Let radar coordinates be represented as ($\rho$, $\theta$), 
and camera coordinates as (u, v), as described in fig \ref{fig:homography_figure}. 
The rotation homography matrix $H=[h_{ij}]_{ij}$ for the camera with respect to the radar, 
originating from the origin and $\omega$ is an unknown constant; can be expressed as follows:

\begin{equation}\label{equ:homography_eq}
    \omega
    \begin{bmatrix}
        u\\
        v\\
        1\\
        \end{bmatrix}=
    \begin{bmatrix}  
        h_{11} & h_{12} & h_{13}\\
        h_{21} & h_{22} & h_{23}\\
        h_{31} & h_{32} & h_{33}\\
    \end{bmatrix}  
    \begin{bmatrix}
    {\rho}\\
    {\theta}\\
    1\\
    \end{bmatrix}
\end{equation}
The equation is similar to the linear least square problem,
with enough data collected in the next section, it is solved with Singular Value Decomposition (SVD).
Let $\omega=1$ Solving equation \ref*{equ:homography_eq} from table \ref*{tab:cam_radar_data} yields :
\begin{equation}\label{equ:H_eq}
H = 
\begin{bmatrix}  
    h_{11} & h_{12} & h_{13}\\
    h_{21} & h_{22} & h_{23}\\
    h_{31} & h_{32} & h_{33}\\
\end{bmatrix}
=
\begin{bmatrix}  
    0 & 0.0003 & -0.2878\\
    0.0002 & 0 & -0.151\\
    0 & 0 & 1\\
\end{bmatrix}
\end{equation}
%\vspace{-5mm}
Some assumptions have to be made to solve the equation:
\vspace{-5mm}
\begin{enumerate}
    \item Radar's plane lies inside the camera plane ($y_r$=0), affine homography is assumed.
    \vspace{-5mm}
    \item Sensor uncertainty (sensor noise) is not considered
    \vspace{-5mm}
    \item This mapping method is only able to accurately project from image coordinates to azimuth in polar coordinates, 
            which is expected
\end{enumerate}
%\subsection{Camera Intrinsic Calibration}
%To accurately map the monocular camera's image coordinates to real-world coordinates, calibration of intrinsic and extrinsic is required.
%Using tools provided by ROS \cite{cam_calib} as shown in figure \ref*{fig:camera_calibration}.

%\begin{figure}[hpbt]
%    \centering
%    \includegraphics[width=6cm]{Figures/cam_calib.png}%\textwidth
%    \caption{Camera extrinsic and intrinsic calibration}
%    \label{fig:camera_calibration}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\subsection{Radar-Camera Transformation Matrix}
While the camera and radar are physically housed as closely as possible, 
as illustrated in Figure \ref{fig:radar_camera_setup_fig}, 
theoretical discrepancies exist.
Accurate fusion of both sensors requires the transformation of their measurements. 
In this scenario, given the challenges in calibrating radar elevation and camera depth, 
a two-dimensional Euclidean space is employed.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=6cm]{Figures/transformation.png}%\textwidth
    \caption{Two rigid body Transformation in 2D coordinate system}
    \label{fig:transformation_figure}
\end{figure}
Let radar coordinates be represented as (x, y), 
and camera coordinates as (x', y'). 
The rotation matrix for the camera with respect to the radar, 
originating from the origin, can be expressed as follows:
\begin{equation}\label{equ:2d_rotation_eq}
    R=
    \begin{bmatrix}
        i_1 & j_1\\
        i_2 & j_2\\
        \end{bmatrix}=
    \begin{bmatrix}
    cos\theta & -sin\theta\\
    sin\theta & cos\theta\\
    \end{bmatrix}
\end{equation}
To relate an arbitrary point in radar coordinates (x, y) to camera coordinates (x', y'), 
the following vector can be employed:
\begin{equation}\label{equ:2d_vector_eq}
    \begin{bmatrix}
        x'\\ 
        y'\\
    \end{bmatrix}=
    \mathbf{t}+x\mathbf{i}+y\mathbf{j}=\mathbf{t}+R
    \begin{bmatrix}
    x\\
    y\\
    \end{bmatrix}
\end{equation}
After encoding the rotation matrix $R$ and the vector, the resulting transformation equation is as follows:
\begin{equation}\label{equ:2d_trans_eq}
    \begin{bmatrix}
        x'\\ 
        y'\\
        1\\
    \end{bmatrix}=
    \begin{bmatrix}
    cos\theta & -sin\theta & t_x\\
    sin\theta & cos\theta & t_y\\
    0 & 0 & 1\\
    \end{bmatrix}
    \begin{bmatrix}
        x\\ 
        y\\
        1\\
    \end{bmatrix}
\end{equation}

\newpage
\subsection{Radar-Camera Data Correlation}
In order to achieve accurate sensor fusion, it is essential to conduct proper calibration of the two sensors. 
For this purpose, a corner reflector is employed (figure \ref*{fig:radar_camera_calibration} \subref{subfig:corner_reflector_fig}), primarily due to its strong radar reflection characteristics(white point in figure \ref*{fig:radar_camera_calibration} \subref{subfig:radar_view_fig}). 
Additionally, it offers the advantage of appearing as a single point in both radar and camera data, effectively reducing ambiguity (figure \ref*{fig:radar_camera_calibration} \subref{subfig:camera_view_fig}).
\subsubsection{Camera to Polar Coordinate Projection}
%Image to Polar Coordinate
The only plane of intersection between the camera and radar in polar coordinates is characterized by theta. 
To integrate the camera data into radar measurements, 
we convert the center of the object detection result into polar coordinates. 
Figure \ref{fig:camera_projection} illustrates the relationship of theta within the camera's plane. 
Equation \ref{equ:img2cart} demonstrates the conversion of pixels to Cartesian coordinates.
\begin{figure}[hpbt]
    \centering
    \includegraphics[width=8cm]{Figures/cam_projection.png}%\textwidth
    \caption{Camera to real-world projection}
    \label{fig:camera_projection}
\end{figure}

\begin{equation}\label{equ:img2cart}
u=c_x-\tan^{-1}(\frac{p_x}{p_y})f
\end{equation}
where
\begin{align*}
    c_x &=\text{center of camera image in pixel}\\
    p_x &=\text{x-axis position in cartesian}\\
    p_y &=\text{y-axis position in cartesian}\\
    f &=\text{camera focal length in pixel}\\
    u &=\text{center of Bbox object detection in pixel}
\end{align*}

Since fusion happens in polar coordinates, we need to rearrange the equation to convert camera's horizontal pixel coordinate into theta.
\begin{equation}\label{equ:2_cam_px}
    \theta_{cam}=
    \tan^{-1}(\frac{p_x}{p_y})=
    \frac
    {(c_x-u)}
    {f}
\end{equation}

This graph shows the comparison between ground truth and homography results.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=10cm]{Figures/matlab/homography_result.png}%\textwidth
    \caption{Homography result of Camera u to Radar Azimuth Data}
    \label{fig:homography_result}
\end{figure}

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=5cm]{Figures/corner_reflector.jpg}%\textwidth
    \caption{Corner Reflector}
    \label{fig:corner_reflector_fig}
\end{figure}
\begin{figure}[hpbt]
    \centering
    \includegraphics[width=5cm]{Figures/camera_corner.png}%\textwidth
    \caption{Corner reflector measurement from camera}
    \label{fig:camera_view_fig}
\end{figure}
\begin{figure}[hpbt]
    \centering
    \includegraphics[width=5cm]{Figures/radar_corner.png}%\textwidth
    \caption{Corner reflector reading from radar (strongest intensity is white)}
    \label{fig:radar_view_fig}
\end{figure}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Calibration Approach}\label{subsec:2-1-data_collect}

Data points from both radar and camera coordinates were collected with the corner reflector positioned at various locations.
After data is collected, radar points are associated with image points based on equation \ref*{equ:homography_eq}.
Image data was collected manually by reading the coordinates of the corner reflector from the camera's image.
The mmWave radar was set to operate at 77-81 GHz chirp, 
with a configuration balanced between range and resolution, 
collecting data at 20 frames per second. 
As shown, the corner reflector represents the radar's reading of the strongest reflection (white dot).
\begin{figure}[hbpt]
    \centering
    \begin{subfigure}{0.25\linewidth}
        \includegraphics[width=5.5cm]{Figures/corner_reflector.jpg}
        \caption{Radar Corner Reflector}
        \label{subfig:corner_reflector_fig}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\linewidth}
        \centering
        \includegraphics[width=5.5cm]{Figures/camera_corner.png}
        \caption{Corner reflector measurement from camera}
        \label{subfig:camera_view_fig}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.25\linewidth}
        \centering
        \includegraphics[width=5.5cm]{Figures/radar_corner.png}
        \caption{Corner reflector reading from radar}
        \label{subfig:radar_view_fig}
    \end{subfigure}

    \caption{Radar camera calibration}
    \label{fig:radar_camera_calibration}
\end{figure}


\newpage
\subsection{Calibration Result}\label{subsec:2-3-calibration-result}
Calibration with the homography and approach produces a respectable result, 
it is able to accurately translate the image coordinate into polar coordinate.
Albeit, there is a limitation to only being able to translate the angle, due to the camera's limitation perceiving depth.
%0.03420363041
\begin{table}[h!]
    \begin{center}
      \label{tab:table4}
      \begin{tabular}{l|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
        \textbf{Method} & \textbf{RMSE} \\% %object detected
        \hline
        Proposed                            & 0.0465 rad, 0.0271 m \\%& 6m 
        \citeauthor{8794186}\cite{8794186}  & 0.025 m \\%
      \end{tabular}
    \end{center}
    \caption{calibration method RMSE compared}
    \label{tab:callib_rmse}
  \end{table}

The table below shows the raw data collected from the camera and radar, 
which is then compared to the homography result of transforming image coordinates 
to polar coordinates for azimuth.
\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{u} & \textbf{v }  & \textbf{radar azimuth $\theta$} & \textbf{camera azimuth} & \textbf{abs error} \\ 
    (pixel)   &(pixel)          &(rad)                  &homography result (rad)&(rad)\\%from eq \ref{equ:2_cam_px}
    \hline
        639 & 297 & 0.03125508785 & 0.001141552511 & 0.03011353534 \\
        589 & 299 & 0.06254076689 & 0.05821917808 & 0.004321588807 \\
        564 & 303 & 0.09388787093 & 0.08675799087 & 0.007129880062 \\
        493 & 304 & 0.1886163813 & 0.1678082192 & 0.02080816213 \\
        433 & 306 & 0.2526802453 & 0.2363013699 & 0.01637887542 \\
        356 & 307 & 0.3178237065 & 0.3242009132 & 0.006377206754 \\
        240 & 317 & 0.4183463717 & 0.4566210046 & 0.03827463284 \\
        200 & 315 & 0.4528166082 & 0.502283105 & 0.04946649681 \\
        100 & 316 & 0.5235987508 & 0.6164383562 & 0.09283960532 \\
        23 & 321 & 0.5600753183 & 0.7043378995 & 0.1442625813 \\
        973 & 230 & -0.3178237119 & -0.3801369863 & 0.06231327441 \\
        945 & 241 & -0.2850964515 & -0.348173516 & 0.06307706449 \\
        885 & 270 & -0.252680245 & -0.2796803653 & 0.02700012035 \\
        848 & 293 & -0.1886163813 & -0.2374429224 & 0.04882654106 \\
        801 & 318 & -0.1568928728 & -0.1837899543 & 0.02689708157 \\
        390 & 272 & 0.2850964456 & 0.2853881279 & 0.0002916822178 \\
        432 & 281 & 0.252680245 & 0.2374429224 & 0.01523732258 \\
        467 & 289 & 0.2205332655 & 0.1974885845 & 0.02304468102 \\
        461 & 299 & 0.2205332494 & 0.2043378995 & 0.01619534981 \\
        485 & 313 & 0.1886163867 & 0.1769406393 & 0.01167574748 \\
        512 & 317 & 0.1568928748 & 0.1461187215 & 0.01077415335 \\
        515 & 328 & 0.1568928728 & 0.1426940639 & 0.01419880884 \\
        533 & 330 & 0.125327834 & 0.1221461187 & 0.00318171529 \\
        1103 & 265 & -0.4528165732 & -0.5285388128 & 0.07572223958 \\
        875 & 282 & -0.2205332494 & -0.2682648402 & 0.04773159083 \\
        \hline
    \end{tabular}
    \caption{Camera data converted into polar coordinate compared to radar measurements (RMSE 0.0465 rad)}
    \label{tab:cam_radar_data}
    \end{table}
\newpage
\subsection{Generating 3D BBox}\label{subsec:2-4-generating-bbox}
The camera's bounding box detection result is initially expressed in pixels. 
To transform these pixel coordinates into a meaningful real-world 3D bounding box, 
it is essential to acquire the distance of the object, a measurement obtained from the radar. 
Once the distance to the object is known, 
the width of the object can be readily derived using homography and trigonometry, 
and converted into cartesian coordinates.
\begin{figure}[hpbt]
    \centering
    \includegraphics[width=8cm]{Figures/cart_width.png}%\textwidth
    \caption{Object's width to polar coordinates relation}
    \label{fig:cart_width}
\end{figure}

Cartesian distance to the object is obtained from range ($\rho$) data from radar's polar coordinate:
\begin{equation}\label{equ:2_py_rho}
    p_y=\rho {sin(\theta)}
    %\frac
    %{cos(\theta)}
    %{\rho}
\end{equation}

Where $x_{min}$,$x_{max}$,$y_{min}$, and $y_{max}$ the size of the bbox in pixel obtained from Yolo detection,
is then converted  into $\theta_{x-max}$ and $\theta_{x-min}$ using homography equation.
\begin{equation}\label{equ:2_cam_width}
    \begin{split}
    \text{width}&=P_{x-max}-P_{x-min}\\
                &=p_y \tan(\theta_{x-max}) - p_y \tan(\theta_{x-min})\\
                &=p_y (\tan(\theta_{x-max}) - \tan(\theta_{x-min}))
    \end{split}
\end{equation}

The same equation can also be applied to derive the height of the detected image. 
\begin{equation}\label{equ:2_cam_height}
    \begin{split}
    \text{height}&=P_{y-max}-P_{y-min}\\
                &=p_y \tan(\theta_{y-max}) - p_y \tan(\theta_{y-min})\\
                &=p_y (\tan(\theta_{y-max}) - \tan(\theta_{y-min}))
    \end{split}
\end{equation}






\section{Data Pre-Processing}\label{sec:2-preprocessing}
\subsection{mmWave Radar Data Pre-Processing}\label{sec:2-kd_tree}
Given that radar data is inherently sparse and noisy, its data needed to be filtered.
For this purpose, k-d tree is employed to cluster the pointcloud.
A k-d tree, short for k-dimensional tree, is a hierarchical data structure used for efficient multidimensional data organization and search operations. 
It arranges data points in k-dimensional space, such as spatial coordinates, in a binary tree structure. 

\subsection{Image Data Pre-Processing}\label{sec:2-img_recognition}
\subsubsection{Image Recognition and Tracking}
YOLOv3 is utilized to generate bounding boxes (BBox) and regions of interest (ROI)\cite{redmon2018yolov3}.
Subsequently, DeepSORT is applied for tracking the generated bounding boxes for objects, 
enabling robust and efficient object tracking throughout the analysis\cite{Wojke2017simple}.

\section{Radar-camera Data Association}\label{sec:2-association}
Prior to fusion, radar clusters have to be associated with tracked objects from deepsort.
First, centroids of radar clusters are mapped into image coordinates.
Second, is to find the theoretical error of radar's measurement \cite{8844649}, which is radar's resolution, determined by equation \ref*{equ:angular_resolution}
Finally, if the distance between the calculated radar centroid and the image detection BBox lies within the theoretical boundary
, we can safely assume both measurements belong to the same object of interest.
\begin{equation}\label{equ:angular_resolution}
    \Delta \theta= \frac{c_0}{f_c d N_{RX} N_{TX} \cos(\theta _i)}
\end{equation}
where
\begin{align*}
    f_c & = \text{center frequency} \\
    \lambda & = \text{carrier signal wavelength} \\
    d & =  \lambda/2 \\
    N_{RX} & = \text{Number of receiving antenna}\\
    N_{TX}& = \text{Number of transferring antenna}\\
    \theta _i &= \text{angle of interest}
\end{align*}

\section{Radar-camera Data Synchronization}\label{sec:2-sync}
Given the disparate update rates of radar and camera data, 
coupled with the inherent time required for data processing, 
effective synchronization of measurements becomes crucial. 
Notably, image processing emerges as the most time-intensive step in this synchronization process. 
The algorithm strategically utilizes radar data corresponding to the processed image, storing it for fusion in later stages, 
as depicted in Figure \ref{fig:sync_radar_cam}.
\begin{figure}[hpbt]
    \centering
    \includegraphics[width=\textwidth]{Figures/syncing.png}%\textwidth
    \caption{Camera and Radar Data Synchronizing}
    \label{fig:sync_radar_cam}
\end{figure}





