%----------------------------------------------------------------------
% BIBLIOGRAPHY (參考書目)
%----------------------------------------------------------------------
%comprehensive review
@ARTICLE{10225711,
  author={Yao, Shanliang and Guan, Runwei and Huang, Xiaoyu and Li, Zhuoxiao and Sha, Xiangyu and Yue, Yong and Lim, Eng Gee and Seo, Hyungjoon and Man, Ka Lok and Zhu, Xiaohui and Yue, Yutao},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review}, 
  year={2024},
  volume={9},
  number={1},
  pages={2094-2128},
  keywords={Radar;Sensors;Radar cross-sections;Cameras;Radar antennas;Tensors;Radar imaging;Autonomous driving;radar-camera fusion;object detection;semantic segmentation},
  doi={10.1109/TIV.2023.3307157}}

%another review
@Article{su14095114,
AUTHOR = {Zhou, Yong and Dong, Yanyan and Hou, Fujin and Wu, Jianqing},
TITLE = {Review on Millimeter-Wave Radar and Camera Fusion Technology},
JOURNAL = {Sustainability},
VOLUME = {14},
YEAR = {2022},
NUMBER = {9},
ARTICLE-NUMBER = {5114},
URL = {https://www.mdpi.com/2071-1050/14/9/5114},
ISSN = {2071-1050},
ABSTRACT = {Cameras allow for highly accurate identification of targets. However, it is difficult to obtain spatial position and velocity information about a target by relying solely on images. The millimeter-wave radar (MMW radar) sensor itself easily acquires spatial position and velocity information of the target but cannot identify the shape of the target. MMW radar and camera, as two sensors with complementary strengths, have been heavily researched in intelligent transportation. This article examines and reviews domestic and international research techniques for the definition, process, and data correlation of MMW radar and camera fusion. This article describes the structure and hierarchy of MMW radar and camera fusion, it also presents its fusion process, including spatio-temporal alignment, sensor calibration, and data information correlation methods. The data fusion algorithms from MMW radar and camera are described separately from traditional fusion algorithms and deep learning based algorithms, and their advantages and disadvantages are briefly evaluated.},
DOI = {10.3390/su14095114}
}

%another review
@misc{wei2022mmwave,
      title={MmWave Radar and Vision Fusion for Object Detection in Autonomous Driving: A Review}, 
      author={Zhiqing Wei and Fengkai Zhang and Shuo Chang and Yangyang Liu and Huici Wu and Zhiyong Feng},
      year={2022},
      eprint={2108.03004},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%data fusion ex
@misc{bansal2022radsegnet,
      title={RadSegNet: A Reliable Approach to Radar Camera Fusion}, 
      author={Kshitiz Bansal and Keshav Rungta and Dinesh Bharadia},
      year={2022},
      eprint={2208.03849},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
%feature ex
@misc{chadwick2019distant,
      title={Distant Vehicle Detection Using Radar and Vision}, 
      author={Simon Chadwick and Will Maddern and Paul Newman},
      year={2019},
      eprint={1901.10951},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

%object fusion ex
@INPROCEEDINGS{8711717,
  author={Jha, Harimohan and Lodhi, Vaibhav and Chakravarty, Debashish},
  booktitle={2019 6th International Conference on Signal Processing and Integrated Networks (SPIN)}, 
  title={Object Detection and Identification Using Vision and Radar Data Fusion System for Ground-Based Navigation}, 
  year={2019},
  volume={},
  number={},
  pages={590-593},
  keywords={Cameras;Radar imaging;Radar detection;Navigation;Sensors;Radar tracking;Radar;Vision;Fusion;Navigation},
  doi={10.1109/SPIN.2019.8711717}}


%multimodal kf
@inproceedings{7472511,
  author={Bourrier, Anthony and Amblard, Pierre-Olivier and Michel, Olivier and Jutten, Christian},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Multimodal Kalman filtering}, 
  year={2016},
  volume={},
  number={},
  pages={4413-4417},
  doi={10.1109/ICASSP.2016.7472511}}

%extending
@ARTICLE{8844649,
  author={Zhang, Renyuan and Cao, Siyang},
  journal={IEEE Access}, 
  title={Extending Reliability of mmWave Radar Tracking and Detection via Fusion With Camera}, 
  year={2019},
  volume={7},
  number={},
  pages={137065-137079},
  doi={10.1109/ACCESS.2019.2942382}}

%trade offs REVIEW
@article{Yao_2023,
	doi = {10.1109/tiv.2023.3307157},
	url = {https://doi.org/10.1109%2Ftiv.2023.3307157},
	year = 2023,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	pages = {1--40},  
	author = {Shanliang Yao and Runwei Guan and Xiaoyu Huang and Zhuoxiao Li and Xiangyu Sha and Yong Yue and Eng Gee Lim and Hyungjoon Seo and Ka Lok Man and Xiaohui Zhu and Yutao Yue},
	title = {Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review}, 
	journal = {{IEEE} Transactions on Intelligent Vehicles}
}

%calibration method compared
@INPROCEEDINGS{8581329,
  author={Oh, Jiyong and Kim, Ki-Seok and Park, Miryong and Kim, Sungho},
  booktitle={2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)}, 
  title={A Comparative Study on Camera-Radar Calibration Methods}, 
  year={2018},
  volume={},
  number={},
  pages={1057-1062},
  keywords={Calibration;Radar imaging;Cameras;Sensors;Optimization;Autonomous vehicles},
  doi={10.1109/ICARCV.2018.8581329}
}
%calib PI
@Article{s110908992,
AUTHOR = {Wang, Tao and Zheng, Nanning and Xin, Jingmin and Ma, Zheng},
TITLE = {Integrating Millimeter Wave Radar with a Monocular Vision Sensor for On-Road Obstacle Detection Applications},
JOURNAL = {Sensors},
VOLUME = {11},
YEAR = {2011},
NUMBER = {9},
PAGES = {8992--9008},
URL = {https://www.mdpi.com/1424-8220/11/9/8992},
PubMedID = {22164117},
ISSN = {1424-8220},
ABSTRACT = {This paper presents a systematic scheme for fusing millimeter wave (MMW) radar and a monocular vision sensor for on-road obstacle detection. As a whole, a three-level fusion strategy based on visual attention mechanism and driver’s visual consciousness is provided for MMW radar and monocular vision fusion so as to obtain better comprehensive performance. Then an experimental method for radar-vision point alignment for easy operation with no reflection intensity of radar and special tool requirements is put forward. Furthermore, a region searching approach for potential target detection is derived in order to decrease the image processing time. An adaptive thresholding algorithm based on a new understanding of shadows in the image is adopted for obstacle detection, and edge detection is used to assist in determining the boundary of obstacles. The proposed fusion approach is verified through real experimental examples of on-road vehicle/pedestrian detection. In the end, the experimental results show that the proposed method is simple and feasible.},
DOI = {10.3390/s110908992}
}


%homography
@article{KIM2014641,
title = {Data fusion of radar and image measurements for multi-object tracking via Kalman filtering},
journal = {Information Sciences},
volume = {278},
pages = {641-652},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.03.080},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514003715},
author = {Du Yong Kim and Moongu Jeon},
keywords = {Kalman filter, Data fusion, Multi-object tracking},
abstract = {Data fusion is an important issue for object tracking in autonomous systems such as robotics and surveillance. In this paper, we present a multiple-object tracking system whose design is based on multiple Kalman filters dealing with observations from two different kinds of physical sensors. Hardware integration which combines a cheap radar module and a CCD camera has been developed and data fusion method has been proposed to process measurements from those modules for multi-object tracking. Due to the limited resolution of bearing angle measurements of the cheap radar module, CCD measurements are used to compensate for the low angle resolution. Conversely, the radar module provides radial distance information which cannot be measured easily by the CCD camera. The proposed data fusion enables the tracker to efficiently utilize the radial measurements of objects from the cheap radar module and 2D location measurements of objects in image space of the CCD camera. To achieve the multi-object tracking we combine the proposed data fusion method with the integrated probability data association (IPDA) technique underlying the multiple-Kalman filter framework. The proposed complementary system based on the radar and CCD camera is experimentally evaluated through a multi-person tracking scenario. The experimental results demonstrate that the implemented system with fused observations considerably enhances tracking performance over a single sensor system.}
}
%another calib procedure
@INPROCEEDINGS{4632220,
  author={Zhengping Ji and Prokhorov, Danil},
  booktitle={2008 11th International Conference on Information Fusion}, 
  title={Radar-vision fusion for object classification}, 
  year={2008},
  volume={},
  number={},
  pages={1-7},
  keywords={automotive;radar;camera;attention selection;sparse representation;MILN},
  doi={}}


%bayes fusion
@InProceedings{10.1007/978-981-16-2248-9_32,
author="Kunjumon, Reshma
and Sangeetha Gopan, G. S.",
editor="Sheth, Amit
and Sinhal, Amit
and Shrivastava, Abhinav
and Pandey, Amit Kumar",
title="Sensor Fusion of Camera and Lidar Using Kalman Filter",
booktitle="Intelligent Systems",
year="2021",
publisher="Springer Singapore",
address="Singapore",
pages="327--343",
abstract="Self-driving cars are the next milestone of the automation industry. To achieve the level of autonomy expected in a self-driving car, the vehicle needs to be mounted with an assortment of sensors that can help the vehicle to perceive its 3D environment better which leads to better decision making and control of the vehicle. To complement the advantages of different sensors, sensor fusion is done, to enhance the accuracy of the overall information. In real-time implementations, uncertainty in factors that affect the vehicle's motion can lead to overshoot in parameters. To avoid that, an estimation filter is used to predict and update the fused values. This paper focuses on sensor fusion of Lidar and Camera followed by estimation using Kalman filter. It can be seen how the use of an estimation filter can significantly improve the accuracy in tracking the path of an obstacle.",
isbn="978-981-16-2248-9"
}

@online{cam_calib,
    author  = {Rukie},
    title   = {How to Calibrate a Monocular Camera},
    year    = {2019},
    month   = {9},
    url     = {https://wiki.ros.org/camera_calibration/Tutorials/MonocularCalibration},
    urldate = {2023-11-04}
}

%deepsort
@inproceedings{Wojke2017simple,
  title={Simple Online and Realtime Tracking with a Deep Association Metric},
  author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
  year={2017},
  pages={3645--3649},
  organization={IEEE},
  doi={10.1109/ICIP.2017.8296962}
}

%yolov3
@misc{redmon2018yolov3,
      title={YOLOv3: An Incremental Improvement}, 
      author={Joseph Redmon and Ali Farhadi},
      year={2018},
      eprint={1804.02767},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


%%%%%%%%%%%%METHODSSSS%%%%%%%%%%

@inproceedings{method1,
author = {Kim, Jinhyeong and Kim, Youngseok and Kum, Dongsuk},
year = {2020},
month = {11},
pages = {},
title = {Low-level Sensor Fusion for 3D Vehicle Detection using Radar Range-Azimuth Heatmap and Monocular Image}
}


@INPROCEEDINGS{8932892,
  author={Zewge, Natnael S. and Kim, Youngmin and Kim, Jintae and Kim, Jong-Hwan},
  booktitle={2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA)}, 
  title={Millimeter-Wave Radar and RGB-D Camera Sensor Fusion for Real-Time People Detection and Tracking}, 
  year={2019},
  volume={},
  number={},
  pages={93-98},
  doi={10.1109/RITAPP.2019.8932892}}
@ARTICLE{9081940,
  author={Kang, Daejun and Kum, Dongsuk},
  journal={IEEE Access}, 
  title={Camera and Radar Sensor Fusion for Robust Vehicle Localization via Vehicle Part Localization}, 
  year={2020},
  volume={8},
  number={},
  pages={75223-75236},
  doi={10.1109/ACCESS.2020.2985075}}



%similar to me, challenging 
@INPROCEEDINGS{8932892,
  author={Zewge, Natnael S. and Kim, Youngmin and Kim, Jintae and Kim, Jong-Hwan},
  booktitle={2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA)}, 
  title={Millimeter-Wave Radar and RGB-D Camera Sensor Fusion for Real-Time People Detection and Tracking}, 
  year={2019},
  volume={},
  number={},
  pages={93-98},
  doi={10.1109/RITAPP.2019.8932892}}



%related work lidar radar
@article{abs180711264,
  author       = {Hatem Hajri and
                  Mohamed{-}Cherif Rahal},
  title        = {Real Time Lidar and Radar High-Level Fusion for Obstacle Detection
                  and Tracking with evaluation on a ground truth},
  journal      = {CoRR},
  volume       = {abs/1807.11264},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.11264},
  eprinttype    = {arXiv},
  eprint       = {1807.11264},
  timestamp    = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-11264.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%related work lidar radar
@INPROCEEDINGS{7579940,
  author={Kwon, Seong Kyung and Hyun, Eugin and Lee, Jin-Hee and Lee, Jonghun and Son, Sang Hyuk},
  booktitle={2016 IEEE 22nd International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)}, 
  title={A Low-Complexity Scheme for Partially Occluded Pedestrian Detection Using LIDAR-RADAR Sensor Fusion}, 
  year={2016},
  volume={},
  number={},
  pages={104-104},
  doi={10.1109/RTCSA.2016.20}}
%calibration
@INPROCEEDINGS{8794186,
  author={Domhof, Joris and Kooij, Julian F.P. and Gavrila, Dariu M.},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={An Extrinsic Calibration Tool for Radar, Camera and Lidar}, 
  year={2019},
  volume={},
  number={},
  pages={8107-8113},
  doi={10.1109/ICRA.2019.8794186}}

%related work lidar camera
@misc{chen2017multiview,
      title={Multi-View 3D Object Detection Network for Autonomous Driving}, 
      author={Xiaozhi Chen and Huimin Ma and Ji Wan and Bo Li and Tian Xia},
      year={2017},
      eprint={1611.07759},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
%related work lidar camera
@misc{li2016vehicle,
      title={Vehicle Detection from 3D Lidar Using Fully Convolutional Network}, 
      author={Bo Li and Tianlei Zhang and Tian Xia},
      year={2016},
      eprint={1608.07916},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
%kalman
@article{kalman,
    author = {Kalman, R. E.},
    title = "{A New Approach to Linear Filtering and Prediction Problems}",
    journal = {Journal of Basic Engineering},
    volume = {82},
    number = {1},
    pages = {35-45},
    year = {1960},
    month = {03},
    abstract = "{The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.}",
    issn = {0021-9223},
    doi = {10.1115/1.3662552},
    url = {https://doi.org/10.1115/1.3662552},
    eprint = {https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/82/1/35/5518977/35\_1.pdf},
}

@INPROCEEDINGS{4732695,
  author={Kmiotek, Pawel and Ruichek, Yassine},
  booktitle={2008 11th International IEEE Conference on Intelligent Transportation Systems}, 
  title={Representing and Tracking of Dynamics Objects using Oriented Bounding Box and Extended Kalman Filter}, 
  year={2008},
  volume={},
  number={},
  pages={322-328},
  doi={10.1109/ITSC.2008.4732695}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  @article{9123408,
    author  = {Guo, Tao and Tian, Chao and Liu, Tie and Yeung, Raymond W.},
    journal = {IEEE Transactions on Information Theory},
    title   = {Weakly Secure Symmetric Multilevel Diversity Coding},
    year    = {2020},
    volume  = {66},
    number  = {11},
    pages   = {7033-7055},
    doi     = {10.1109/TIT.2020.3004482}
}

@online{1111111,
    author  = {Google},
    title   = {尼卡諾爾帕拉 107 歲冥誕},
    year    = {2021},
    month   = {Sep},
    url     = {https://www.google.com/doodles/nicanor-parras-107th-birthday},
    urldate = {2021-09-05}
}


% Technical Specification 
@techreport{25300,
    author      = {3GPP},
    institution = {{3rd Generation Partnership Project (3GPP)}},
    month       = {Jul.},
    note        = {Version 12.6.0},
    number      = {25.300},
    title       = {{Universal Terrestrial Radio Access Network (UTRAN); General description; Stage 2}},
    type        = { Technical Specification (TS)},
    url         = {https://www.3gpp.org/DynaReport/25300.htm},
    year        = {2016}
}

% Technical Report
@techreport{45001,
    author      = {3GPP},
    institution = {{3rd Generation Partnership Project (3GPP)}},
    month       = {Apr.},
    note        = {Version 4.5.0},
    number      = {45.001},
    title       = {{GSM/EDGE Physical layer on the radio path; General description}},
    type        = { Technical Report (TR)},
    url         = {https://www.3gpp.org/DynaReport/45001.htm},
    year        = {2005}
}

@inbook{101002,
    publisher = {{John Wiley \& Sons, Ltd}},
    isbn      = {9781119556060},
    title     = {Sifting Through Services},
    booktitle = {CompTIA Linux+ Study Guide},
    chapter   = {2},
    pages     = {15-38},
    doi       = {https://doi.org/10.1002/9781119556060.ch2},
    url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119556060.ch2},
    eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119556060.ch2},
    year      = {2019},
    keywords  = {Linux servers, software packages, network clients, Linux kernel, personal programs, command-line interface (CLI)}
}